{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from modules import components, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELANE_NAME = \"stroke-pipeline\"\n",
    "\n",
    "# Pipeline inputs\n",
    "DATA_ROOT = \"data\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
    "TUNER_MODULE_FILE = \"modules/tuner.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
    "\n",
    "# Pipeline outputs\n",
    "OUTPUT_BASE = \"outputs\"\n",
    "\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, \"serving_model\")\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELANE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menjalankan ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 17s]\n",
      "binary_accuracy: 0.9559687376022339\n",
      "\n",
      "Best binary_accuracy So Far: 0.9594218730926514\n",
      "Total elapsed time: 00h 09m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in outputs\\stroke-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\kt_hyperband\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000019C21F670D0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 176\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0003\n",
      "Score: 0.9594218730926514\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "dense_unit: 176\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.9577343463897705\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 176\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9575937390327454\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "dense_unit: 144\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.9563124775886536\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 48\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.956250011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 112\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.9559687376022339\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "dense_unit: 144\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9546093940734863\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 176\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9525312781333923\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "dense_unit: 176\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9517655968666077\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 144\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9514374732971191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " gender_xf (InputLayer)         [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " ever_married_xf (InputLayer)   [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " work_type_xf (InputLayer)      [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " Residence_type_xf (InputLayer)  [(None, 3)]         0           []                               \n",
      "                                                                                                  \n",
      " smoking_status_xf (InputLayer)  [(None, 5)]         0           []                               \n",
      "                                                                                                  \n",
      " age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " hypertension_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " heart_disease_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " avg_glucose_level_xf (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 24)           0           ['gender_xf[0][0]',              \n",
      "                                                                  'ever_married_xf[0][0]',        \n",
      "                                                                  'work_type_xf[0][0]',           \n",
      "                                                                  'Residence_type_xf[0][0]',      \n",
      "                                                                  'smoking_status_xf[0][0]',      \n",
      "                                                                  'age_xf[0][0]',                 \n",
      "                                                                  'hypertension_xf[0][0]',        \n",
      "                                                                  'heart_disease_xf[0][0]',       \n",
      "                                                                  'avg_glucose_level_xf[0][0]']   \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 176)          4400        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 176)          31152       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 176)          31152       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 176)          31152       ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            177         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 98,033\n",
      "Trainable params: 98,033\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      " 997/1000 [============================>.] - ETA: 0s - loss: 0.1517 - binary_accuracy: 0.9529\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.94573, saving model to outputs\\stroke-pipeline\\Trainer\\model\\8\\Format-Serving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Residence_type_xf with unsupported characters which will be renamed to residence_type_xf in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\stroke-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\stroke-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 23s 20ms/step - loss: 0.1516 - binary_accuracy: 0.9529 - val_loss: 0.2119 - val_binary_accuracy: 0.9457\n",
      "Epoch 2/2\n",
      " 997/1000 [============================>.] - ETA: 0s - loss: 0.1240 - binary_accuracy: 0.9577\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.94573\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 0.1240 - binary_accuracy: 0.9577 - val_loss: 0.2956 - val_binary_accuracy: 0.9428\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\stroke-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\stroke-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C21FD93A0> and <keras.engine.input_layer.InputLayer object at 0x0000019C218A0580>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C21FD93A0> and <keras.engine.input_layer.InputLayer object at 0x0000019C218A0580>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C42199A00> and <keras.engine.input_layer.InputLayer object at 0x0000019C2EAB1A00>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C42199A00> and <keras.engine.input_layer.InputLayer object at 0x0000019C2EAB1A00>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C32145280> and <keras.engine.input_layer.InputLayer object at 0x0000019C321EEDC0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C32145280> and <keras.engine.input_layer.InputLayer object at 0x0000019C321EEDC0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C1AFE8A90> and <keras.engine.input_layer.InputLayer object at 0x0000019C3AFA9850>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C1AFE8A90> and <keras.engine.input_layer.InputLayer object at 0x0000019C3AFA9850>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C2EDE6D30> and <keras.engine.input_layer.InputLayer object at 0x0000019C3B0E4BB0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C2EDE6D30> and <keras.engine.input_layer.InputLayer object at 0x0000019C3B0E4BB0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C477D8C70> and <keras.engine.input_layer.InputLayer object at 0x0000019C477B9B20>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C477D8C70> and <keras.engine.input_layer.InputLayer object at 0x0000019C477B9B20>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C47D6C490> and <keras.engine.input_layer.InputLayer object at 0x0000019C47DEE220>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C47D6C490> and <keras.engine.input_layer.InputLayer object at 0x0000019C47DEE220>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C48374FA0> and <keras.engine.input_layer.InputLayer object at 0x0000019C482F8B50>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000019C48374FA0> and <keras.engine.input_layer.InputLayer object at 0x0000019C482F8B50>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Zis\\anaconda3\\envs\\stroke-pred\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Zis\\anaconda3\\envs\\stroke-pred\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    }
   ],
   "source": [
    "components = components.init_components(\n",
    "    DATA_ROOT,\n",
    "    trainer_module=TRAINER_MODULE_FILE,\n",
    "    tuner_module=TUNER_MODULE_FILE,\n",
    "    transform_module=TRANSFORM_MODULE_FILE,\n",
    "    train_steps=1000,\n",
    "    eval_steps=300,\n",
    "    serving_model_dir=serving_model_dir,\n",
    ")\n",
    "\n",
    "pipeline = pipeline.init_pipeline(\n",
    "    pipeline_root, \n",
    "    PIPELANE_NAME, \n",
    "    metadata_path, \n",
    "    components\n",
    ")\n",
    "BeamDagRunner().run(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('stroke-pred')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "493718c44ded53567b94188f49dffc3801c9a9345e39599a35702a1d13d18025"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
